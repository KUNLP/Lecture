{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9"
      },
      "source": [
        "<h1>개인 구글 드라이브와 colab 연동</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4X-lNQfT1D1v"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jj3zBDGhp3mY"
      },
      "outputs": [],
      "source": [
        "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4B-rbK8b3TW",
        "outputId": "57c1b95f-16a7-4e9f-eeb4-6ecd3d3e4bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# !pip install sentencepiece\n",
        "\n",
        "root_dir = \"/gdrive/My Drive/colab/4-2. Pretrained LM1\"\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWZLECjoJkn"
      },
      "source": [
        "<h1>BERT 모델을 이용한 감성분류</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o5861idd25QB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "\n",
        "class SentimentClassifier(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(SentimentClassifier, self).__init__(config)\n",
        "\n",
        "        # BERT 모델\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # 히든 사이즈\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # 분류할 라벨의 개수\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.linear = nn.Linear(in_features=self.hidden_size, out_features=self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "\n",
        "        # (batch_size, max_length, hidden_size)\n",
        "        bert_output = outputs[0]\n",
        "\n",
        "        # (batch_size, hidden_size)\n",
        "        cls_vector = bert_output[:, 0, :]\n",
        "\n",
        "        # class_output : (batch_size, num_labels)\n",
        "        cls_output = self.linear(cls_vector)\n",
        "\n",
        "        return cls_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caSL4RyA1OOm"
      },
      "source": [
        "<h1>데이터 읽고 전처리 하기</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path)</b>\n",
        "  \"train_datas_wordpiece.txt\", \"test_datas_wordpiece.txt\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  데이터 예시)\n",
        "    ▁아 ▁더 빙 . . ▁진짜 ▁짜 증 나 네요 ▁목소리 \\t negative\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    datas : 영화 리뷰, 정답 라벨\n",
        "    \n",
        "    출력 예시)\n",
        "      datas = [\n",
        "        (['▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네요', '▁목소리'], negative)\n",
        "\n",
        "        (...),\n",
        "        \n",
        "        ]\n",
        "      \n",
        "<b>2. read_vocab_data(vocab_data_path)</b>\n",
        "  \"label_vocab.txt\" 파일을 읽고 라벨을 indexing하기 위한 딕셔너리를 생성\n",
        "   \n",
        "  read_vocab_data(vocab_data_path)\n",
        "  args\n",
        "    vocab_data_path : 어휘 파일 경로\n",
        "  return  \n",
        "    term2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2term : index를 대응하는 라벨로 치환하기 위한 딕셔너리\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx)</b>\n",
        "  입력 데이터를 고정된 길이로 변환 후 indexing\n",
        "  Tensor로 변환\n",
        "   \n",
        "  convert_data2feature(datas, max_length, tokenizer, label2idx)\n",
        "  args\n",
        "    datas : 영화 리뷰 데이터와 대응하는 정답 라벨을 갖고 있는 리스트\n",
        "    max_length : 입력의 최대 길이\n",
        "    tokenizer : electra tokenizer 객체\n",
        "    label2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "  return\n",
        "    input_ids_features : 입력 문장에 대한 index sequence\n",
        "    label_id_features : 정답을 갖고 있는 리스트\n",
        "    \n",
        "  전처리 예시)\n",
        "    tokenizing된 리뷰 데이터['▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네요', '▁목소리', ...]\n",
        "    input_ids : [2, 3360, 28709, 18, 18, 12704, 29334, ... ]\n",
        "    label_id : [1]\n",
        " </pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iOGS8rse1ZZZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    datas = []\n",
        "    for line in lines:\n",
        "        # 입력 데이터를 \\t을 기준으로 분리\n",
        "        pieces = line.strip().split(\"\\t\")\n",
        "\n",
        "        # 리뷰, 정답\n",
        "        input_sequence, label = pieces[0].split(\" \"), pieces[1]\n",
        "\n",
        "        datas.append((input_sequence, label))\n",
        "\n",
        "    return datas\n",
        "\n",
        "\n",
        "def read_vocab_data(vocab_data_path):\n",
        "    term2idx, idx2term = {}, {}\n",
        "\n",
        "    with open(vocab_data_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        term = line.strip()\n",
        "        term2idx[term] = len(term2idx)\n",
        "        idx2term[term2idx[term]] = term\n",
        "\n",
        "    return term2idx, idx2term\n",
        "\n",
        "\n",
        "\n",
        "def convert_data2feature(datas, max_length, tokenizer, label2idx):\n",
        "    input_ids_features, label_id_features = [], []\n",
        "\n",
        "    for input_sequence, label in datas:\n",
        "\n",
        "        # CLS, SEP 토큰 추가\n",
        "        tokens = [tokenizer.cls_token]\n",
        "        tokens += input_sequence\n",
        "        tokens = tokens[:max_length - 1]\n",
        "        tokens += [tokenizer.sep_token]\n",
        "\n",
        "        # word piece들을 대응하는 index로 치환\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # padding 생성\n",
        "        padding = [tokenizer._convert_token_to_id(tokenizer.pad_token)] * (max_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "\n",
        "        label_id = label2idx[label]\n",
        "\n",
        "        # 변환한 데이터를 각 리스트에 저장\n",
        "        input_ids_features.append(input_ids)\n",
        "        label_id_features.append(label_id)\n",
        "\n",
        "\n",
        "    # 변환한 데이터를 Tensor 객체에 담아 반환\n",
        "    input_ids_features = torch.tensor(input_ids_features, dtype=torch.long)\n",
        "    label_id_features = torch.tensor(label_id_features, dtype=torch.long)\n",
        "\n",
        "    return input_ids_features, label_id_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLIOIHT779c"
      },
      "source": [
        "<h1>BERT 모델 학습</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>2. read_vocab_data(vocab_data_path) 함수를 사용하여 어휘 딕셔너리 생성</b>\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx) 함수를 사용하여 데이터 전처리</b>\n",
        "\n",
        "<b>4. BERT 모델 객체 선언 후 사전 학습 파일 불러옴</b>\n",
        "\n",
        "<b>5. epoch 마다 학습한 모델 파일 저장</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rsYLc2YK8eNc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, TensorDataset, RandomSampler)\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertConfig\n",
        "# from tokenization_kobert import KoBertTokenizer\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    # BERT config 객체 생성\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                             cache_dir=config[\"cache_dir_path\"])\n",
        "    setattr(bert_config, \"num_labels\", config[\"num_labels\"])\n",
        "\n",
        "    # BERT tokenizer 객체 생성\n",
        "    bert_tokenizer = KoBERTTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                     cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # 라벨 딕셔너리 생성\n",
        "    label2idx, idx2label = read_vocab_data(vocab_data_path=config[\"label_vocab_data_path\"])\n",
        "\n",
        "    # 학습 및 평가 데이터 읽기\n",
        "    train_datas = read_data(file_path=config[\"train_data_path\"])\n",
        "    train_datas = train_datas[:5000]\n",
        "\n",
        "    # 입력 데이터 전처리\n",
        "    train_input_ids_features, train_label_id_features = convert_data2feature(datas=train_datas,\n",
        "                                                                             max_length=config[\"max_length\"],\n",
        "                                                                             tokenizer=bert_tokenizer,\n",
        "                                                                             label2idx=label2idx)\n",
        "\n",
        "    # 학습 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    train_dataset = TensorDataset(train_input_ids_features, train_label_id_features)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config[\"batch_size\"],\n",
        "                                  sampler=RandomSampler(train_dataset))\n",
        "\n",
        "    # 사전 학습된 BERT 모델 파일로부터 가중치 불러옴\n",
        "    model = SentimentClassifier.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n",
        "\n",
        "    # loss를 계산하기 위한 함수\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 모델 학습을 위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = []\n",
        "        for batch in train_dataloader:\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            input_ids, label_id = batch\n",
        "\n",
        "            # 역전파 단계를 실행하기 전에 변화도를 0으로 변경\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 모델 예측 결과\n",
        "            hypothesis = model(input_ids)\n",
        "\n",
        "            # loss 계산\n",
        "            loss = loss_func(hypothesis, label_id)\n",
        "\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            # batch 단위 loss 값 저장\n",
        "            total_loss.append(loss.data.item())\n",
        "\n",
        "        bert_config.save_pretrained(save_directory=config[\"output_dir_path\"])\n",
        "        model.save_pretrained(save_directory=config[\"output_dir_path\"])\n",
        "\n",
        "        print(\"Average loss : {}\".format(np.mean(total_loss)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20OC-TY8FIFj"
      },
      "source": [
        "<h1>BERT 모델 평가</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>2. read_vocab_data(vocab_data_path) 함수를 사용하여 어휘 딕셔너리 생성</b>\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx) 함수를 사용하여 데이터 전처리</b>\n",
        "\n",
        "<b>4. BERT 모델 객체 선언 후 기존에 학습한 모델 파일 불러옴</b>\n",
        "\n",
        "<b>5. 학습한 BERT 모델 평가</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ORSC_y9Nto04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, TensorDataset, SequentialSampler)\n",
        "\n",
        "from transformers import BertConfig\n",
        "# from tokenization_kobert import KoBertTokenizer\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "\n",
        "def test(config):\n",
        "    # BERT config 객체 생성\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"output_dir_path\"],\n",
        "                                             cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # BERT tokenizer 객체 생성 (기존 BERT tokenizer 그대로 사용)\n",
        "    bert_tokenizer = KoBERTTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                     cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # 라벨 딕셔너리 생성\n",
        "    label2idx, idx2label = read_vocab_data(vocab_data_path=config[\"label_vocab_data_path\"])\n",
        "\n",
        "    # 평가 데이터 읽기\n",
        "    test_datas = read_data(file_path=config[\"test_data_path\"])\n",
        "    test_datas = test_datas[:100]\n",
        "\n",
        "    # 입력 데이터 전처리\n",
        "    test_input_ids_features, test_label_id_features = convert_data2feature(datas=test_datas,\n",
        "                                                                           max_length=config[\"max_length\"],\n",
        "                                                                           tokenizer=bert_tokenizer,\n",
        "                                                                           label2idx=label2idx)\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    test_dataset = TensorDataset(test_input_ids_features, test_label_id_features)\n",
        "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config[\"batch_size\"],\n",
        "                                 sampler=SequentialSampler(test_dataset))\n",
        "\n",
        "    # 학습한 모델 파일로부터 가중치 불러옴\n",
        "    model = SentimentClassifier.from_pretrained(pretrained_model_name_or_path=config[\"output_dir_path\"],\n",
        "                                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "        input_ids, label_id = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 모델 예측 결과\n",
        "            hypothesis = model(input_ids)\n",
        "            # 모델의 출력값에 softmax와 argmax 함수를 적용\n",
        "            hypothesis = torch.argmax(torch.softmax(hypothesis, dim=-1), dim=-1)\n",
        "\n",
        "        # Tensor를 리스트로 변경\n",
        "        hypothesis = hypothesis.cpu().detach().numpy().tolist()\n",
        "        label_id = label_id.cpu().detach().numpy().tolist()\n",
        "\n",
        "        for index in range(len(input_ids)):\n",
        "            input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[index])\n",
        "            input_sequence = bert_tokenizer.convert_tokens_to_string(input_tokens[1:input_tokens.index(bert_tokenizer.sep_token)])\n",
        "            predict = idx2label[hypothesis[index]]\n",
        "            correct = idx2label[label_id[index]]\n",
        "\n",
        "            print(\"입력 : {}\".format(input_sequence))\n",
        "            print(\"출력 : {}, 정답 : {}\\n\".format(predict, correct))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk-3ADInQDHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf21099-af7c-4097-93df-2c5a9223d425"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n",
            "Some weights of SentimentClassifier were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['linear.bias', 'linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss : 0.7000699950631257\n",
            "Average loss : 0.6848260103517277\n",
            "Average loss : 0.5423173126141736\n",
            "Average loss : 0.47463378129871026\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    cache_dir = os.path.join(root_dir, \"cache\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "              \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"cache_dir_path\": cache_dir,\n",
        "              \"pretrained_model_name_or_path\": \"skt/kobert-base-v1\",\n",
        "              \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "              \"num_labels\": 2,\n",
        "              \"max_length\": 142,\n",
        "              \"epoch\":10,\n",
        "              \"batch_size\":32\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ofQlbxbGeMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}