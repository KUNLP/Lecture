{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stock_rnn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FdZnDI8azinw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621244345322,"user_tz":-540,"elapsed":24237,"user":{"displayName":"Harksoo Kim","photoUrl":"","userId":"04506968767642445103"}},"outputId":"6601b28e-fed8-42fa-fbe1-5a48ffe8a66a"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-Yj5-JyNCLs"},"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n","import csv\n","from sklearn.preprocessing import MinMaxScaler\n","\n","class STOCK_RNN(nn.Module):\n","\n","  def __init__(self, config):\n","    \n","    super(STOCK_RNN, self).__init__()\n","\n","    self.input_size = config[\"input_size\"]\n","    self.hidden_size = config[\"hidden_size\"]\n","    self.output_size = config[\"output_size\"]\n","    self.num_layers = config[\"num_layers\"]\n","    self.batch_size = config[\"batch_size\"]\n","\n","    # LSTM 설계\n","    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, bidirectional=False, batch_first=True)\n","    # 출력층 설계\n","    self.linear = nn.Linear(self.hidden_size,self.output_size)\n","\n","  def forward(self, input_features):\n","\n","    # LSTM 리턴 = output (배치, 시퀀스, 은닉 상태), (hidden_state, cell_state)\n","    x, (h_n, c_n) = self.lstm(input_features)\n","\n","    # output에서 마지막 시퀀스의 (배치, 은닉 상태) 정보를 가져옴\n","    h_t = x[:,-1,:]\n","\n","    # 출력층: (배치, 출력)\n","    hypothesis = self.linear(h_t)\n"," \n","    return hypothesis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b7WN3AN_R6Q"},"source":["# 데이터 읽기 함수\n","def load_dataset(fname):\n","\n","  f = open(fname, 'r', encoding='cp949')\n","\n","  # CSV 파일 읽기\n","  data = csv.reader(f,delimiter=',')\n","\n","  # 헤더 건너뛰기\n","  next(data)\n","\n","  data_X = []\n","  data_Y = []\n","\n","  for row in data:\n","    # 오픈, 고가, 저가, 거래량 -> 숫자 변환\n","    data_X.append([float(i) for i in row[2:]])\n","    # 종가 -> 숫자 변환\n","    data_Y.append(float(row[1]))\n","\n","  # MinMax 정규화 (예측하려는 종가 제외)\n","  scaler = MinMaxScaler()\n","  scaler.fit(data_X)\n","  data_X = scaler.transform(data_X)\n","\n","  data_num = len(data_X)\n","  sequence_len = config[\"sequence_len\"]\n","  seq_data_X, seq_data_Y = [], []\n","\n","  # 윈도우 크기만큼 슬라이딩 하면서 데이터 생성\n","  for i in range(data_num-sequence_len):\n","    window_size = i+sequence_len\n","    seq_data_X.append(data_X[i:window_size])\n","    seq_data_Y.append([data_Y[window_size-1]])\n","\n","  (train_X, train_Y) = (np.array(seq_data_X[:]),np.array(seq_data_Y[:]))\n","  train_X = torch.tensor(train_X, dtype=torch.float) \n","  train_Y = torch.tensor(train_Y, dtype=torch.float) \n","\n","  print(train_X.shape) # (73,3,4)\n","  print(train_Y.shape) # (73,1)\n","  \n","  return (train_X, train_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fE5aJVyfRdn6"},"source":["# 모델 평가 결과 계산을 위해 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","# 평가 수행 함수\n","def do_test(model, test_dataloader):\n","\n","  # 평가 모드 셋팅\n","  model.eval()\n","\n","  # Batch 별로 예측값과 정답을 저장할 리스트 초기화\n","  predicts, golds = [], []\n","  \n","  with torch.no_grad():\n","\n","    for step, batch in enumerate(test_dataloader):\n","  \n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      input_features, labels = batch\n","      hypothesis = model(input_features)\n","\n","      x = tensor2list(hypothesis[:,0])\n","      y = tensor2list(labels)\n","\n","      # 예측값과 정답을 리스트에 추가\n","      predicts.extend(x)\n","      golds.extend(y)\n","    \n","    # 소숫점 이하 1자리로 변환\n","    predicts = [round(i,1) for i in predicts]\n","    golds = [round(i[0],1) for i in golds]\n","\n","    print(\"PRED=\",predicts)\n","    print(\"GOLD=\",golds)\n","\n","# 모델 평가 함수\n","def test(config):\n","\n","  model = STOCK_RNN(config).cuda()\n","\n","  # 저장된 모델 가중치 로드\n","  model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"model_name\"])))\n","\n","  # 데이터 load\n","  (features, labels) = load_dataset(confing[\"file_name\"])\n","  \n","  test_features = TensorDataset(features, labels)\n","  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n","  \n","  do_test(model, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f7fNFnA_Lhe"},"source":["# 모델 학습 함수\n","def train(config):\n","\n","  # 모델 생성\n","  model = STOCK_RNN(config).cuda()\n","\n","  # 데이터 읽기\n","  (input_features, labels) = load_dataset(config[\"file_name\"])\n","  \n","  # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n","  train_features = TensorDataset(input_features, labels)\n","  train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","  # MSE (Mean Square Error) 비용 함수 \n","  loss_func = nn.MSELoss()\n","  # 옵티마이저 함수 (역전파 알고리즘을 수행할 함수)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n","\n","  for epoch in range(config[\"epoch\"]+1):\n","\n","    # 학습 모드 셋팅\n","    model.train()\n","\n","    # epoch 마다 평균 비용을 저장하기 위한 리스트\n","    costs = []\n","\n","    for (step, batch) in enumerate(train_dataloader):\n","\n","      # batch = (input_features[step], labels[step])*batch_size\n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      # 각 feature 저장\n","      input_features, labels = batch\n","\n","      # 역전파 변화도 초기화\n","      # .backward() 호출 시, 변화도 버퍼에 데이터가 계속 누적한 것을 초기화\n","      optimizer.zero_grad()\n","\n","      # H(X) 계산: forward 연산\n","      hypothesis = model(input_features)\n","      # 비용 계산\n","      cost = loss_func(hypothesis, labels)\n","      # 역전파 수행\n","      cost.backward()\n","      optimizer.step()\n","   \n","      # 현재 batch의 스텝 별 loss 저장\n","      costs.append(cost.data.item())\n","    \n","    # 에폭마다 평균 비용 출력하고 모델을 저장\n","    print(\"Average Loss= {0:f}\".format(np.mean(costs)))\n","    torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))\n","    do_test(model, train_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wbpqmro-xhg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621248275344,"user_tz":-540,"elapsed":4452,"user":{"displayName":"Harksoo Kim","photoUrl":"","userId":"04506968767642445103"}},"outputId":"6bc594f0-e923-44cf-df63-1d693cbdfd2a"},"source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/colab/rnn/stock\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"file_name\": \"{0:s}/samsung-2020.csv\".format(root_dir),\n","              \"sequence_len\": 3,\n","              \"input_size\": 4,\n","              \"hidden_size\": 10,\n","              \"output_size\": 1,\n","              \"num_layers\": 1,\n","              \"batch_size\": 1,\n","              \"learn_rate\": 0.1,\n","              \"epoch\": 10,\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([72, 3, 4])\n","torch.Size([72, 1])\n","Average Loss= 907.702039\n","PRED= [53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1]\n","GOLD= [45.4, 47.3, 55.9, 60.7, 43.0, 56.5, 59.9, 61.8, 57.9, 58.8, 50.0, 60.0, 55.4, 60.0, 45.4, 56.4, 58.6, 60.4, 60.4, 60.5, 47.8, 62.3, 59.7, 58.9, 47.8, 56.5, 55.5, 62.4, 57.4, 59.2, 50.0, 47.0, 60.8, 48.3, 61.5, 59.8, 42.5, 48.3, 56.8, 59.5, 60.8, 61.4, 56.8, 61.1, 61.3, 56.5, 60.0, 54.6, 61.8, 59.1, 59.5, 54.2, 45.6, 61.3, 48.7, 57.2, 54.2, 57.2, 54.2, 57.8, 59.5, 50.8, 59.2, 59.0, 55.8, 60.7, 56.4, 55.0, 48.9, 52.1, 55.5, 60.2]\n","Average Loss= 30.854506\n","PRED= [54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5, 54.5]\n","GOLD= [61.3, 54.6, 56.5, 54.2, 50.8, 60.7, 58.9, 54.2, 61.8, 61.1, 42.5, 60.0, 56.5, 61.5, 57.4, 47.8, 59.7, 60.5, 59.5, 59.9, 60.8, 58.6, 56.8, 61.4, 45.4, 47.3, 59.8, 59.2, 59.5, 56.4, 56.5, 55.9, 50.0, 48.3, 57.2, 62.4, 62.3, 57.2, 55.0, 43.0, 60.0, 55.5, 47.0, 57.9, 48.3, 48.9, 45.4, 59.1, 59.0, 59.5, 61.3, 55.5, 60.7, 47.8, 45.6, 57.8, 55.4, 52.1, 61.8, 60.4, 60.2, 50.0, 60.4, 56.4, 58.8, 54.2, 55.8, 48.7, 59.2, 56.8, 60.0, 60.8]\n","Average Loss= 29.129469\n","PRED= [55.7, 55.8, 55.3, 55.9, 56.0, 56.2, 55.7, 56.1, 56.0, 55.5, 55.4, 55.9, 55.8, 56.1, 55.7, 56.1, 55.5, 55.4, 56.2, 55.7, 55.4, 55.8, 55.4, 55.4, 55.7, 56.0, 55.6, 55.9, 56.1, 56.0, 56.1, 55.8, 55.4, 56.2, 56.1, 56.2, 56.0, 56.0, 55.9, 56.0, 56.0, 56.0, 56.0, 55.4, 55.8, 56.1, 56.2, 56.0, 55.4, 55.9, 56.1, 55.7, 55.7, 55.7, 56.2, 55.4, 55.8, 55.4, 55.7, 56.1, 55.8, 56.1, 56.2, 55.9, 55.7, 55.5, 56.1, 55.9, 56.1, 56.1, 55.7, 55.7]\n","GOLD= [55.5, 59.5, 42.5, 61.1, 56.4, 60.8, 55.0, 59.1, 59.5, 45.6, 43.0, 57.2, 54.2, 60.4, 58.6, 61.8, 48.9, 47.8, 62.3, 58.9, 48.7, 59.5, 48.3, 48.3, 52.1, 60.0, 55.4, 56.4, 59.7, 60.5, 59.2, 54.2, 47.3, 60.8, 60.4, 58.8, 61.3, 59.9, 57.9, 57.2, 59.0, 60.0, 56.8, 45.4, 57.8, 60.7, 62.4, 61.3, 45.4, 55.9, 60.0, 56.8, 50.0, 55.8, 61.8, 47.0, 54.2, 47.8, 55.5, 59.8, 56.5, 61.4, 60.2, 56.5, 57.4, 50.0, 61.5, 56.5, 60.7, 59.2, 54.6, 50.8]\n","Average Loss= 12.220411\n","PRED= [58.1, 58.1, 58.1, 58.0, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 46.8, 46.7, 58.1, 47.7, 46.7, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 46.7, 58.1, 46.9, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 46.8, 58.1, 58.1, 58.1, 46.8, 58.1, 46.8, 58.1, 58.1, 46.9, 58.1, 58.1, 46.8, 58.1, 58.1, 58.1, 46.7, 58.1, 58.1, 58.1, 58.1, 58.1, 57.4, 46.7, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 58.1, 46.8, 58.1]\n","GOLD= [57.4, 55.8, 60.2, 50.8, 58.9, 60.7, 57.8, 54.6, 59.5, 61.5, 62.4, 56.8, 61.4, 61.8, 60.4, 48.3, 47.0, 59.9, 50.0, 42.5, 54.2, 60.7, 58.6, 55.9, 56.4, 62.3, 45.4, 59.5, 47.3, 59.7, 57.9, 55.5, 60.0, 56.5, 61.3, 59.8, 47.8, 60.4, 59.5, 60.5, 48.9, 56.5, 48.3, 56.4, 54.2, 45.6, 56.8, 55.4, 45.4, 59.2, 56.5, 52.1, 48.7, 58.8, 60.8, 61.8, 55.0, 60.8, 50.0, 47.8, 57.2, 60.0, 59.0, 59.1, 60.0, 57.2, 59.2, 54.2, 55.5, 61.1, 43.0, 61.3]\n","Average Loss= 7.938679\n","PRED= [58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 53.4, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.5, 47.1, 58.7, 58.7, 58.7, 47.2, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 48.5, 58.7, 58.7, 47.2, 58.7, 58.7, 58.7, 58.5, 58.7, 47.2, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 47.1, 58.7, 58.7, 58.7, 58.7, 47.2, 51.5, 58.7, 47.2, 58.7, 47.4, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 48.3, 58.6, 58.7]\n","GOLD= [55.4, 60.0, 61.3, 59.2, 56.8, 59.5, 57.2, 60.0, 48.9, 58.8, 55.8, 59.2, 60.4, 55.5, 60.8, 56.4, 60.5, 55.0, 60.7, 47.3, 42.5, 61.4, 55.9, 60.4, 45.4, 55.5, 59.8, 56.4, 56.5, 61.5, 62.4, 48.3, 57.9, 59.9, 47.0, 56.5, 57.8, 57.4, 50.0, 60.2, 47.8, 52.1, 58.9, 59.1, 54.2, 59.7, 56.5, 48.7, 59.0, 54.2, 61.3, 60.0, 45.4, 45.6, 59.5, 43.0, 50.8, 47.8, 62.3, 61.8, 60.7, 54.6, 57.2, 61.1, 61.8, 54.2, 56.8, 58.6, 59.5, 48.3, 50.0, 60.8]\n","Average Loss= 9.703581\n","PRED= [58.9, 58.9, 47.2, 47.4, 58.9, 58.8, 58.9, 58.8, 58.8, 58.9, 58.9, 58.8, 58.8, 58.8, 58.9, 58.8, 58.9, 47.3, 58.9, 58.9, 58.9, 58.9, 58.8, 58.9, 58.8, 58.8, 47.2, 47.3, 47.2, 58.8, 47.6, 58.8, 58.8, 58.8, 58.8, 47.2, 58.9, 58.8, 58.7, 58.8, 58.8, 47.2, 58.8, 47.3, 58.8, 58.8, 58.8, 58.7, 58.8, 56.4, 47.2, 58.8, 47.1, 58.8, 58.8, 58.9, 58.8, 58.8, 58.8, 58.8, 58.7, 47.2, 58.8, 58.8, 47.2, 58.8, 58.9, 58.8, 47.2, 58.9, 58.7, 58.8]\n","GOLD= [59.1, 61.5, 50.0, 47.3, 62.4, 59.2, 62.3, 56.4, 58.6, 60.7, 58.8, 60.4, 56.5, 60.0, 61.8, 55.4, 59.2, 48.3, 59.9, 61.4, 60.2, 60.8, 56.8, 59.8, 59.5, 60.4, 45.6, 48.3, 48.9, 60.7, 50.0, 60.5, 59.7, 55.9, 55.5, 47.8, 60.8, 60.0, 55.0, 56.4, 54.6, 45.4, 56.8, 47.0, 57.2, 56.5, 61.3, 57.4, 61.3, 50.8, 45.4, 55.5, 42.5, 59.5, 59.5, 59.0, 58.9, 56.5, 54.2, 55.8, 52.1, 48.7, 54.2, 57.2, 47.8, 61.8, 60.0, 61.1, 43.0, 57.9, 57.8, 54.2]\n","Average Loss= 6.190605\n","PRED= [59.3, 59.3, 59.3, 59.3, 47.8, 47.9, 59.3, 59.3, 59.3, 59.3, 59.3, 59.0, 47.9, 48.8, 59.3, 59.3, 59.3, 59.3, 59.3, 47.7, 47.9, 59.3, 59.2, 47.9, 58.6, 48.2, 47.9, 47.8, 59.3, 57.9, 48.0, 59.3, 47.8, 59.3, 59.3, 59.3, 59.3, 59.3, 59.3, 47.9, 59.3, 59.3, 59.2, 58.8, 59.3, 59.3, 59.3, 47.9, 47.9, 59.3, 59.3, 59.3, 59.3, 59.3, 48.0, 59.3, 59.3, 59.3, 59.3, 59.3, 59.3, 59.3, 59.3, 59.3, 47.8, 59.3, 59.3, 59.3, 59.3, 59.3, 59.2, 59.3]\n","GOLD= [59.2, 59.1, 58.8, 60.8, 45.4, 48.3, 60.0, 58.9, 62.3, 60.0, 57.2, 55.4, 47.8, 52.1, 60.8, 60.4, 59.2, 56.8, 61.4, 42.5, 48.3, 61.3, 54.2, 50.0, 55.0, 50.8, 45.6, 43.0, 55.5, 54.6, 50.0, 61.3, 48.9, 55.9, 60.7, 58.6, 55.8, 59.5, 56.8, 45.4, 56.5, 60.2, 54.2, 57.4, 56.4, 61.8, 60.4, 47.0, 47.8, 60.0, 57.8, 57.9, 61.1, 59.8, 47.3, 56.4, 61.8, 59.5, 60.5, 59.5, 59.9, 55.5, 61.5, 59.7, 48.7, 56.5, 62.4, 56.5, 57.2, 60.7, 54.2, 59.0]\n","Average Loss= 6.076716\n","PRED= [46.9, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 57.5, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 50.7, 58.3, 47.1, 46.9, 46.9, 58.3, 47.0, 58.3, 58.3, 47.1, 58.3, 58.3, 47.0, 58.2, 58.2, 58.3, 47.0, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 47.4, 58.3, 58.3, 58.3, 58.2, 47.1, 58.3, 58.3, 46.8, 58.3, 58.3, 58.3, 47.0, 58.3, 58.3, 58.3, 58.3, 46.9, 58.3, 46.9, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 58.3, 47.0, 58.3]\n","GOLD= [47.8, 60.0, 55.5, 56.5, 60.0, 56.5, 56.4, 52.1, 60.4, 59.1, 61.3, 54.2, 55.5, 62.4, 50.8, 59.2, 48.3, 48.7, 45.4, 57.2, 47.8, 56.4, 60.0, 48.3, 59.0, 57.8, 45.6, 55.0, 57.4, 60.8, 50.0, 61.8, 60.4, 59.5, 56.5, 54.2, 59.5, 59.2, 61.3, 50.0, 60.7, 61.1, 60.8, 54.6, 47.3, 58.6, 61.5, 42.5, 54.2, 56.8, 57.9, 45.4, 59.7, 59.8, 55.9, 60.5, 48.9, 60.7, 43.0, 58.8, 57.2, 61.4, 59.9, 55.8, 55.4, 60.2, 58.9, 59.5, 62.3, 56.8, 47.0, 61.8]\n","Average Loss= 8.436461\n","PRED= [58.7, 47.3, 58.7, 58.7, 58.6, 58.6, 58.4, 47.1, 47.0, 58.6, 47.1, 54.1, 58.6, 58.6, 58.7, 58.7, 58.7, 58.6, 58.7, 58.7, 58.7, 58.7, 58.6, 58.5, 58.6, 58.7, 58.5, 58.7, 58.7, 58.7, 58.7, 58.5, 58.7, 58.6, 58.7, 58.6, 58.7, 58.6, 58.7, 49.1, 58.7, 58.7, 58.7, 58.6, 58.6, 58.7, 58.7, 47.2, 47.0, 47.3, 58.6, 58.5, 58.6, 58.7, 58.7, 47.0, 58.7, 58.7, 58.7, 58.7, 58.7, 58.7, 58.6, 58.6, 58.4, 58.6, 58.7, 47.1, 49.0, 58.7, 58.6, 47.1]\n","GOLD= [60.4, 48.9, 60.0, 60.7, 55.9, 56.8, 47.3, 47.8, 43.0, 56.5, 47.0, 48.3, 57.8, 56.4, 61.3, 62.4, 60.0, 59.5, 58.9, 58.8, 60.2, 60.4, 56.5, 55.0, 57.2, 60.8, 50.8, 59.2, 60.0, 56.4, 55.8, 52.1, 59.0, 55.5, 60.8, 58.6, 62.3, 55.5, 61.4, 48.3, 59.9, 60.5, 56.8, 59.5, 59.5, 59.1, 61.8, 45.6, 45.4, 47.8, 55.4, 57.4, 61.1, 59.8, 59.2, 42.5, 61.3, 57.9, 59.7, 57.2, 60.7, 61.8, 56.5, 54.2, 50.0, 54.2, 54.6, 45.4, 50.0, 61.5, 54.2, 48.7]\n","Average Loss= 9.082568\n","PRED= [57.9, 57.9, 49.9, 47.2, 57.5, 57.7, 57.9, 57.8, 57.9, 57.8, 57.7, 57.7, 46.3, 57.8, 57.8, 57.9, 57.9, 57.8, 57.8, 57.7, 57.9, 57.8, 57.9, 57.9, 56.6, 51.9, 57.9, 46.3, 46.7, 54.4, 57.8, 57.9, 57.7, 57.9, 46.4, 57.8, 57.8, 57.8, 57.7, 57.7, 57.6, 57.9, 57.7, 57.9, 46.3, 57.8, 57.7, 57.9, 57.8, 46.4, 57.9, 57.7, 57.8, 57.8, 57.9, 57.8, 57.9, 57.9, 57.8, 57.9, 57.9, 57.9, 57.9, 57.7, 57.9, 57.9, 57.8, 57.7, 57.7, 57.9, 46.6, 57.9]\n","GOLD= [59.9, 59.1, 47.8, 48.7, 48.3, 55.9, 60.4, 61.3, 61.8, 56.4, 47.3, 54.2, 45.4, 56.4, 61.1, 60.8, 59.7, 56.8, 57.2, 52.1, 61.8, 55.5, 59.8, 59.2, 48.3, 50.0, 61.5, 42.5, 47.8, 48.9, 54.2, 58.9, 54.2, 62.4, 47.0, 60.0, 58.6, 56.8, 50.8, 59.5, 50.0, 62.3, 56.5, 60.2, 43.0, 60.4, 57.8, 60.8, 56.5, 45.4, 54.6, 55.0, 55.4, 59.5, 60.7, 57.2, 60.7, 61.3, 55.8, 57.9, 59.0, 60.5, 60.0, 56.5, 59.2, 60.0, 55.5, 57.4, 59.5, 58.8, 45.6, 61.4]\n","Average Loss= 10.367217\n","PRED= [58.6, 47.3, 58.3, 58.7, 58.7, 58.9, 47.2, 58.7, 58.8, 58.7, 47.2, 58.9, 58.8, 58.8, 47.2, 58.8, 58.9, 58.7, 58.9, 47.2, 58.9, 58.7, 47.4, 58.8, 47.2, 47.2, 58.9, 58.8, 58.8, 47.2, 47.2, 58.6, 58.8, 47.2, 58.7, 58.9, 58.7, 58.8, 47.3, 58.9, 47.3, 58.7, 58.8, 58.7, 58.8, 58.6, 49.0, 58.8, 58.7, 58.9, 47.1, 58.9, 58.8, 58.8, 58.8, 47.2, 58.8, 58.8, 58.8, 58.9, 58.8, 58.9, 58.7, 58.7, 58.9, 58.7, 58.8, 58.9, 58.7, 58.8, 58.8, 58.7]\n","GOLD= [55.0, 47.3, 52.1, 57.2, 55.9, 59.1, 47.0, 54.2, 55.8, 54.2, 45.6, 60.8, 60.4, 58.9, 47.8, 60.5, 60.2, 59.5, 62.4, 42.5, 57.9, 55.5, 50.0, 59.7, 48.9, 45.4, 60.0, 58.6, 56.4, 43.0, 45.4, 57.4, 61.8, 47.8, 61.1, 62.3, 54.2, 61.8, 48.3, 61.4, 48.3, 56.5, 57.2, 56.5, 60.0, 57.8, 50.8, 56.4, 55.4, 60.8, 48.7, 60.7, 60.7, 54.6, 60.4, 50.0, 61.3, 61.3, 56.8, 61.5, 59.2, 59.8, 55.5, 59.5, 59.9, 56.8, 59.0, 58.8, 56.5, 60.0, 59.2, 59.5]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Npu0G6ajsZKG"},"source":[""],"execution_count":null,"outputs":[]}]}