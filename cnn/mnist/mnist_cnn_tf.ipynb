{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"F24pbMKw8uT9","colab_type":"text"},"source":["<h2>개인 구글 드라이브와 colab 연동</h2>"]},{"cell_type":"code","metadata":{"id":"4X-lNQfT1D1v","colab_type":"code","outputId":"f9b49fa4-b988-4c28-b366-2c956b118fad","executionInfo":{"status":"ok","timestamp":1570194887937,"user_tz":-540,"elapsed":15255,"user":{"displayName":"최기현","photoUrl":"","userId":"07435435166393705635"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H85SqgIZkEPs","colab_type":"text"},"source":["<h2>CNN 모델</h2>\n","\n","![대체 텍스트](http://nlp.kangwon.ac.kr/~nlpdemo/cnn.png)"]},{"cell_type":"code","metadata":{"id":"o5861idd25QB","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","class CNN(object):\n","  \n","  def __init__(self, flags):\n","    self.learning_rate = flags[\"learning_rate\"]  # 학습률\n","    self.num_classes = flags[\"num_classes\"]      # 분류할 클래스의 개수\n","    self.mode = flags[\"mode\"]                    # 학습 or 평가 상태\n","\n","    self._input_init()\n","    self._cnn_init()\n","    self._predict_init()\n","    self._train_init()\n","\n","  # 입력 데이터, 정답 데이터, keep_prob 값을 담을 tensor 선언\n","  def _input_init(self):\n","    # 입력 데이터\n","    self.inputs = tf.placeholder(tf.float32, [None, 28, 28], name=\"inputs\")\n","    \n","    # 정답 데이터\n","    self.targets = tf.placeholder(tf.int32, [None, self.num_classes], name=\"targets\")\n","\n","    # 노드를 보전할 확률\n","    self.keep_prob = tf.placeholder(tf.float32, [], \"keep_prob\")\n","\n","\n","  def _cnn_init(self):\n","    with tf.name_scope(\"cnn_layer\"):\n","         \n","      convolution_w = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01), name=\"convolution_w\")\n","\n","      # (?, 28, 28) -> (?, 28, 28, 1)\n","      inputs = tf.expand_dims(input=self.inputs, axis=-1)\n","      \n","      # (?, 28, 28, 1) -> (?, 28, 28, 32)\n","      conv_layer = tf.nn.conv2d(inputs, convolution_w, strides=[1, 1, 1, 1], padding='SAME', name=\"conv_layer\")\n","      # (?, 28, 28, 32) -> (?, 28, 28, 32)\n","      conv_layer = tf.nn.relu(conv_layer)\n","      \n","      # (?, 28, 28, 32) -> (?, 14, 14, 32)\n","      conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n","      # (?, 14, 14, 32) -> (?, 14, 14, 32)\n","      conv_layer = tf.nn.dropout(conv_layer, keep_prob=self.keep_prob)\n","\n","      # (?, 14, 14, 32) -> (?, 6272)\n","      conv_layer = tf.reshape(tensor=conv_layer, shape=[-1, 14*14*32])\n","      \n","      # (?, 6272) -> (?, 10)\n","      fully_connected_layer = tf.layers.dense(inputs=conv_layer, units=10)\n","    \n","      self.outputs = fully_connected_layer\n","\n","  def _predict_init(self):\n","    # 각 클래스의 분포 값들을 softmax 함수를 이용하여 0~1사이의 값으로 변경\n","    self.predict_op = tf.nn.softmax(logits=self.outputs, axis=-1)\n","\n","  def _train_init(self):\n","    if self.mode == \"train\":\n","      with tf.variable_scope(\"train_layer\"):\n","        # 모델의 출력인 self.outputs 와 self.targets를 비교하여 loss 계산\n","        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.targets, logits=self.outputs)\n","        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","        self.grads = self.optimizer.compute_gradients(self.loss)\n","        self.train_op = self.optimizer.apply_gradients(self.grads)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwrnE5li9bmF","colab_type":"text"},"source":["<h2>read_file, get_batch 함수</h2>\n","\n","<pre>\n","<b>1. read_file(file_path)</b>\n","  \"mnist_train.csv\", \"mnist_test.csv\" 파일을 읽기 위한 함수\n","  \n","  read_file(file_path)\n","  args\n","    file_path : 읽고자 하는 데이터의 경로\n","  return\n","    입력 이미지에 대한 픽셀값과 해당 이미지의 라벨을 담고 있는 리스트\n","    \n","  데이터 예시)\n","    1, 0, 0, 0, 0, ... 0.6, 0.8, 0, 0, ... 0.7, 0.1, 0, 0, ... 0, 0, 0\n","    라벨, 픽셀값, 픽셀값, ... 픽셀값\n","</pre>\n","![대체 텍스트](http://nlp.kangwon.ac.kr/~nlpdemo/mnist_1.png)\n","<pre>\n","<b>2. get_batch(datas, batch_size)</b>\n","  전체 데이터를 batch 단위로 나누어 주기 위한 함수\n","  \n","  get_batch(datas, batch_size)\n","  args\n","    datas : 입력 이미지에 대한 픽셀값과 해당 이미지의 라벨로 이루어진 리스트\n","    batch_size : 한번에 학습할 데이터의 개수\n","  return\n","    batch 단위로 나뉘어진 데이터 리스트\n","    \n","  예시) \n","    batch_size = 3 인 경우\n","  \n","    total_datas = [ (입력1, 라벨1), (입력2, 라벨2), (입력3, 라벨3), ... ,(입력10, 라벨10) ]\n","    batches = [\n","    [ (입력1, 라벨1), (입력2, 라벨2), (입력3, 라벨3)],\n","    [ (입력4, 라벨4), (입력5, 라벨5), (입력6, 라벨6)],\n","    [ (입력7, 라벨7), (입력8, 라벨8), (입력9, 라벨9)],\n","    ]\n","</pre>"]},{"cell_type":"code","metadata":{"id":"_hopD20S9VJa","colab_type":"code","colab":{}},"source":["import csv\n","import numpy as np\n","\n","# 데이터를 읽고 픽셀값과 라벨을 분리하여 datas 리스트에 저장\n","def read_file(file_path):\n","    inFile = open(file_path, \"r\", encoding=\"utf8\")\n","    input_file = csv.reader(inFile)\n","\n","    datas = []\n","    for line in input_file:\n","        data, label = np.array(line[1:]), np.array(line[0])\n","        index = int(label)\n","\n","        # 픽셀값을 (28, 28) 행렬로 변환\n","        # 라벨값은 one-hot encoding으로 변환\n","        # 예시) 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","        data, label = np.reshape(data, (28, 28)), np.zeros(shape=(10))\n","        label[index] = 1\n","\n","        datas.append((data, label))\n","\n","    inFile.close()\n","\n","    return datas\n","\n","# 데이터를 batch 단위로 분할하여 저장\n","def get_batch(datas, batch_size):\n","  \n","  # batches : batch 단위로 저장할 리스트\n","  # x_datas : 각 batch 단위 별 입력 데이터\n","  # y_datas : 각 batch 단위 별 정답 데이터\n","  batches, x_datas, y_datas = [], [], []\n","  \n","  for data, label in datas:\n","    x_datas.append(data)\n","    y_datas.append(label)\n","\n","    # x_datas에 담긴 데이터의 개수가 batch_size와 같아지면 \n","    # x_datas와 y_datas를 batches 리스트에 저장하고 x_datas, y_datas는 비움\n","    if(len(x_datas) == batch_size):\n","      batches.append((x_datas, y_datas))\n","      x_datas, y_datas = [], []\n","\n","  return batches"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnULUngdEoJb","colab_type":"text"},"source":["<h2>CNN 모델 학습</h2>\n","\n","<pre>\n","<b>1. read_file(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n","\n","<b>2. CNN 모델 객체 선언</b>\n","\n","<b>3. epoch를 돌때마다 학습 데이터 셔플</b>\n","\n","<b>3. batch 단위로 학습을 수행</b>\n","</pre>"]},{"cell_type":"code","metadata":{"id":"-WPLgRpO9SAP","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from tqdm import tqdm\n","\n","def train(flags):\n","  # 학습 데이터 읽기\n","  train_datas = read_file(flags[\"train_data_path\"])\n","\n","  # 모델 객체 선언\n","  model = CNN(flags)\n","  \n","  # tensorflow session 옵션 설정\n","  # allow_soft_placement=True : 어떤 device를 사용하여 연산할지 명시하지 않은 경우 자동으로 존재하는 디바이스 중에서 하나를 선택\n","  # gpu_options=tf.GPUOptions(allow_growth=True) : 연산 실행 과정에서 필요한만큼의 gpu 메모리만 사용\n","  sess_config = tf.ConfigProto(allow_soft_placement=True, \n","                               gpu_options=tf.GPUOptions(allow_growth=True))\n","\n","  # tensorflow를 실행하기 위한 session \n","  with tf.Session(config=sess_config) as sess:\n","    # 그래프 초기화\n","    sess.run(tf.global_variables_initializer())\n","    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n","    saver = tf.train.Saver(max_to_keep=10)\n","\n","    for epoch in tqdm(range(flags[\"epoch\"])):\n","      # 학습 데이터 셔플\n","      np.random.shuffle(train_datas)\n","      # 학습 데이터를 batch 단위로 분할하여 저장\n","      batches = get_batch(train_datas, flags[\"batch_size\"])\n","\n","      losses = []\n","      # batch 단위로 학습을 진행하며 각 batch 별 loss를 구한다\n","      # batch 별 loss들의 평균을 구하여 이를 전체 데이터에 대한 loss로 사용\n","      for train_x, train_y in batches:\n","          loss, train_op = sess.run([model.loss, model.train_op],\n","                                    feed_dict={ model.inputs:train_x, \n","                                               model.targets:train_y, \n","                                               model.keep_prob:flags[\"keep_prob\"] }\n","                                    )\n","          losses.append(loss)\n","\n","      # 학습한 모델 파일 저장\n","      filename = os.path.join(flags[\"save_dir\"], \"model_{}.ckpt\".format(epoch+1))\n","      saver.save(sess, filename)\n","\n","      print(\"\\tEpoch : {}, Average_Loss : {}\".format(epoch+1, np.mean(losses)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AeYeqCc_MUof","colab_type":"text"},"source":["<h2>CNN 모델 평가</h2>\n","\n","<pre>\n","<b>1. read_file(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n","\n","<b>2. CNN 모델 객체 선언</b>\n","\n","<b>3. tf.train.Saver() 객체를 사용하여 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴</b>\n","</pre>"]},{"cell_type":"code","metadata":{"id":"gPLfTilw9MVF","colab_type":"code","colab":{}},"source":["import numpy as np\n","from tqdm import tqdm\n","\n","def test(flags):\n","  # 평가 데이터 읽기\n","  test_datas = read_file(flags[\"test_data_path\"])\n","\n","  # 모델 객체 선언\n","  model = CNN(flags)\n","  # tensorflow session 옵션 설정\n","  sess_config = tf.ConfigProto(allow_soft_placement=True, \n","                               gpu_options=tf.GPUOptions(allow_growth=True))\n","\n","  # tensorflow를 실행하기 위한 session \n","  with tf.Session(config=sess_config) as sess:\n","    # 그래프 초기화\n","    sess.run(tf.global_variables_initializer())\n","    # 학습 파일을 저장거나 불러오기 위한 saver 객체\n","    saver = tf.train.Saver()\n","    \n","    # 학습한 모델 파일 중에서 가장 많이 학습된 파일로부터 가중치를 불러옴\n","    print(\"Read from : \" + str(tf.train.latest_checkpoint(flags[\"save_dir\"])))\n","    saver.restore(sess, tf.train.latest_checkpoint(flags[\"save_dir\"]))\n","\n","    # 평가 데이터를 batch 단위로 분할하여 저장\n","    batches = get_batch(test_datas, flags[\"batch_size\"])\n","\n","    # 정답을 맞춘 개수\n","    correct_count = 0\n","    for test_x, test_y in tqdm(batches):\n","      predict_op = sess.run(model.predict_op,\n","                            feed_dict={ model.inputs:test_x, model.keep_prob:flags[\"keep_prob\"] }\n","                            )\n","\n","      # 모델의 outputs에는 각 클래스에 대한 분포가 저장되어 있고\n","      # np.argmax 함수를 통하여 가장 확률이 높은 클래스를 선택\n","      # 예시) \n","      #  predict_op = [0,1, 0.3, 0.2] (각각 0일 확률, 1일 확률, 2일 확률)\n","      #  np.argmax(predict_op) = 1\n","      outputs, correct = np.argmax(predict_op, axis=-1), np.argmax(test_y, axis=-1)\n","      correct_count += np.sum(np.equal(outputs, correct))\n","\n","    print(\"Accuracy : \" + str(100.0*correct_count/(len(batches))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SzkPsqYtNBHu","colab_type":"text"},"source":["<h2>모델의 hyper parameter 설정, 학습 및 평가 실행</h2>\n","\n","<pre>\n","root_dir : 코드와 데이터가 있는 디렉토리 경로\n","save_dir : 학습한 모델 파일을 저장할 디렉토리 경로(디렉토리가 존재하지 않을 경우 자동으로 생성)\n","\n","<b>flags : hyper parameter를 저장할 딕셔너리</b>\n","  flags.mode = 학습 또는 평가 설정(\"train\" or \"test\")\n","  flags.save_dir = 학습한 모델 파일을 저장할 디렉토리 경로\n","  flags.batch_size = 한번에 학습할 데이터의 개수\n","  flags.epoch = 학습 횟수\n","  flags.learning_rate = 학습률\n","  flags.keep_prob = 노드를 보전할 확률\n","  flags.num_classes = 분류할 클래스의 개수(0~9, 따라서 10개)\n","  flags.train_data_path = 학습데이터 파일 경로\n","  flags.test_data_path = 평가데이터 파일 경로\n","\n","<b>mode 별 hyper parameter 변경</b>\n","  학습하는 경우 : mode를 \"train\"으로 설정, 나머지는 기본 설정 그대로 유지\n","  평가하는 경우 : mode를 \"test\"로, batch_size는 1로, keep_prob은 1.0으로 변경\n","</pre>"]},{"cell_type":"code","metadata":{"id":"bQfWrdn33utZ","colab_type":"code","outputId":"7ef4db8e-abe8-42e3-d039-89c8099ed5b7","executionInfo":{"status":"ok","timestamp":1570195002614,"user_tz":-540,"elapsed":19374,"user":{"displayName":"최기현","photoUrl":"","userId":"07435435166393705635"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["import os\n","\n","if __name__ == \"__main__\":\n","  root_dir = \"/gdrive/My Drive/colab/cnn\"\n","  save_dir = os.path.join(root_dir, \"model\")\n","  if not os.path.exists(save_dir):\n","      os.makedirs(save_dir)\n","\n","  flags = {\"mode\":\"train\",\n","           \"save_dir\":save_dir,\n","           \"batch_size\":64,\n","           \"epoch\":10,\n","           \"learning_rate\":0.001,\n","           \"keep_prob\":0.7,\n","           \"num_classes\":10,\n","           \"train_data_path\":os.path.join(root_dir, \"mnist_train.csv\"),\n","           \"test_data_path\":os.path.join(root_dir, \"mnist_test.csv\")          \n","          }\n","  \n","  tf.reset_default_graph()\n","  if(flags[\"mode\"] == \"train\"):\n","      train(flags)\n","  elif(flags[\"mode\"] == \"test\"):\n","      flags[\"batch_size\"] = 1  \n","      flags[\"keep_prob\"] = 1.0\n","      test(flags)\n","  else:\n","      print(\"Unknown mode\")\n","      exit(0)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/10000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd4db2856a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd4db2856a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","Read from : /gdrive/My Drive/colab/cnn/model/model_10.ckpt\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:17<00:00, 587.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 97.64\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hPeU03-nvigz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}