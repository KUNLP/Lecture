{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FdZnDI8azinw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620869574856,"user_tz":-540,"elapsed":2269,"user":{"displayName":"Harksoo Kim","photoUrl":"","userId":"04506968767642445103"}},"outputId":"9b00861e-25c2-4676-f940-475deaf00f47"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-Yj5-JyNCLs"},"source":["import os\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n","from keras.datasets import mnist\n","\n","class MNIST(nn.Module):\n","\n","  def __init__(self, config):\n","    super(MNIST, self).__init__()\n","\n","    # 입력층 노드 수\n","    self.inode = config[\"input_node\"]\n","    # 은닉층 데이터 크기\n","    self.hnode = config[\"hidden_node\"]\n","    # 출력층 노드 수: 분류해야 하는 레이블 수\n","    self.onode = config[\"output_node\"]\n","\n","    # 활성화 함수로 Sigmoid 사용\n","    self.activation = nn.Sigmoid()\n","\n","    # 신경망 설계\n","    self.linear1 = nn.Linear(self.inode, self.hnode, bias=True)\n","    self.linear2 = nn.Linear(self.hnode, self.onode, bias=True)  \n","\n","  def forward(self, input_features):\n","\n","    output1 = self.linear1(input_features)\n","    hypothesis1 = self.activation(output1)\n","    \n","    output2 = self.linear2(hypothesis1)\n","    hypothesis2 = self.activation(output2)\n","\n","    return hypothesis2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b7WN3AN_R6Q"},"source":["# 데이터 읽기 함수\n","def load_dataset():\n","\n","  (train_X, train_y), (test_X, test_y) = mnist.load_data()\n","  print(train_X.shape) # (60000, 28, 28)\n","  print(train_y.shape) # (60000,10)\n","  print(test_X.shape) # (10000, 28, 28)\n","  print(test_y.shape) # (10000,10)\n","  \n","  train_X = train_X.reshape(-1, 28*28)\n","  print(train_X.shape)\n","  test_X  = test_X.reshape(-1, 28*28)\n","\n","  train_X = torch.tensor(train_X, dtype=torch.float)\n","  train_y = torch.tensor(train_y, dtype=torch.long)\n","  test_X = torch.tensor(test_X, dtype=torch.float)\n","  test_y = torch.tensor(test_y, dtype=torch.long)\n","  \n","  return (train_X, train_y), (test_X, test_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fE5aJVyfRdn6"},"source":["# 모델 평가 결과 계산을 위해 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","# 평가 수행 함수\n","def do_test(model, test_dataloader):\n","\n","  # 평가 모드 셋팅\n","  model.eval()\n","\n","  # Batch 별로 예측값과 정답을 저장할 리스트 초기화\n","  predicts, golds = [], []\n","  \n","  with torch.no_grad():\n","\n","    for step, batch in enumerate(test_dataloader):\n","  \n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      input_features, labels = batch\n","      hypothesis = model(input_features)\n","\n","      # ont-hot 표현으로 변경\n","      logits = torch.argmax(hypothesis,-1)\n","\n","      x = tensor2list(logits)\n","      y = tensor2list(labels)\n","\n","      # 예측값과 정답을 리스트에 추가\n","      predicts.extend(x)\n","      golds.extend(y)\n","    \n","    print(\"PRED=\",predicts)\n","    print(\"GOLD=\",golds)\n","    print(\"Accuracy= {0:f}\\n\".format(accuracy_score(golds, predicts)))\n","\n","# 모델 평가 함수\n","def test(config):\n","\n","  model = MNIST(config).cuda()\n","\n","  # 저장된 모델 가중치 로드\n","  model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"model_name\"])))\n","\n","  # 데이터 load\n","  (_, _), (features, labels) = load_dataset()\n","  \n","  test_features = TensorDataset(features, labels)\n","  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n","  \n","  do_test(model, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f7fNFnA_Lhe"},"source":["# 모델 학습 함수\n","def train(config):\n","\n","  # 모델 생성\n","  model = MNIST(config).cuda()\n","\n","  # 데이터 읽기\n","  (input_features, labels), (_, _) = load_dataset()\n","\n","  # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n","  train_features = TensorDataset(input_features, labels)\n","  train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","\n","  # 크로스엔트로피 비용 함수 \n","  loss_func = nn.CrossEntropyLoss()\n","  # 옵티마이저 함수 (역전파 알고리즘을 수행할 함수)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n","\n","  for epoch in range(config[\"epoch\"]+1):\n","\n","    # 학습 모드 셋팅\n","    model.train()\n","    \n","    # epoch 마다 평균 비용을 저장하기 위한 리스트\n","    costs = []\n","\n","    for (step, batch) in enumerate(train_dataloader):\n","\n","      # batch = (input_features[step], labels[step])*batch_size\n","      # .cuda()를 통해 메모리에 업로드\n","      batch = tuple(t.cuda() for t in batch)\n","\n","      # 각 feature 저장\n","      input_features, labels = batch\n","\n","      # 역전파 변화도 초기화\n","      # .backward() 호출 시, 변화도 버퍼에 데이터가 계속 누적한 것을 초기화\n","      optimizer.zero_grad()\n","\n","      # H(X) 계산: forward 연산\n","      hypothesis = model(input_features)\n","      # 비용 계산\n","      cost = loss_func(hypothesis, labels)\n","      # 역전파 수행\n","      cost.backward()\n","      optimizer.step()\n","   \n","      # 현재 batch의 스텝 별 loss 저장\n","      costs.append(cost.data.item())\n","    \n","    # 에폭마다 평균 비용 출력하고 모델을 저장\n","    print(\"Average Loss= {0:f}\".format(np.mean(costs)))\n","    torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))\n","    do_test(model, train_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wbpqmro-xhg"},"source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/colab/ann/mnist\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"input_node\":784,\n","              \"hidden_node\":512,\n","              \"output_node\":10,\n","              \"learn_rate\":0.001,\n","              \"batch_size\":32,\n","              \"epoch\":10,\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":null,"outputs":[]}]}